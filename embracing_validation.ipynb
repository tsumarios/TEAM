{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Automated Embracing Approach Validation\n",
    "\n",
    "This notebook provides the workflow within all the steps to validate the Semi-Automated Embracing Approach for (Privacy) Threat Modelling."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "\n",
    "Run the following cells before starting with your validation.\n",
    "\n",
    "*Remember to install dependencies only once, so just comment the cell after executed. On the other hand, module imports need to be done on each run.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from itables import init_notebook_mode\n",
    "\n",
    "from embracing import compute_semantic_similarity_scores, compute_semantic_similarity_scores_between_files\n",
    "\n",
    "init_notebook_mode(all_interactive=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Semantic Similarity Scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preliminary_threat_list_path = f'./data/validation/tool_final_threats.csv'\n",
    "tool_min_score, tool_max_score = compute_semantic_similarity_scores(preliminary_threat_list_path, f'./data/tool_final_threats_ss_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preliminary_threat_list_path = f'./data/validation/vehits_final_threats.csv'\n",
    "validator_min_score, validator_max_score = compute_semantic_similarity_scores(preliminary_threat_list_path, f'./data/vehits_final_threats_ss_scores.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Semantic Similarity Scores for Validation\n",
    "\n",
    "The following code computes the semantic similarity scores between each final threat from the automated list with each threat from the validation list. Then, the maximum value is considered as a metrics of comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_output_path = f'./data/validation/tool_final_threats.csv'\n",
    "validator_source_output_path = f'./data/validation/vehits_final_threats.csv'\n",
    "output_comparison_ss_scores_path = f'./data/validation/comparison_ss_scores.csv'\n",
    "min_score, max_score = compute_semantic_similarity_scores_between_files(tool_output_path, validator_source_output_path, output_comparison_ss_scores_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ss_scores_df = pd.read_csv(f'./data/final_comparison_semantic_similarity_scores.csv')\n",
    "# if 'min_score' not in globals() or 'max_score' not in globals():\n",
    "#     min_score, max_score = ss_scores_df['score'].round(2).min(), ss_scores_df['score'].round(2).max()\n",
    "ss_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density_plot = ss_scores_df.plot.density(title=f'Similarity score range is between {min_score} and {max_score}')\n",
    "density_plot.set_xlim(min_score, max_score)\n",
    "hist_plot = ss_scores_df.plot.hist(title=f'Average similarity score is {ss_scores_df[\"score\"].mean().round(2)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = ss_scores_df.loc[ss_scores_df['score'] <= 0.999999]\n",
    "filtered_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
