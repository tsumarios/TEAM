{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEAM 3\n",
    "\n",
    "This notebook provides the workflow within all the steps that compose the TEAM 3 algorithm within the SPADA methodology for threat modelling."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "\n",
    "Run the following cells before starting with your analysis.\n",
    "\n",
    "*Remember to install dependencies only once, so just comment the cell after executed. On the other hand, module imports need to be done on each run.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from ipywidgets import widgets\n",
    "from itables import init_notebook_mode\n",
    "\n",
    "from embracing_utils import compute_semantic_similarity_scores, merge_csv_files, filter_dataframe_by_threshold, find_synset_relations, synset_relations\n",
    "\n",
    "init_notebook_mode(all_interactive=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Threats Upload and Semantic Similarity Computation\n",
    "\n",
    "Please upload the input threats list in CSV format (suggested path: `./data/TEAM 3/<round_no>/<file>.csv`).\n",
    "\n",
    "Remember to perform this operation at the start of a new round. Just change the `round_no` variable with the name of the current step and the `input_threat_list_path` variable with the path to the threat list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "round_no = 'third'  # CHANGE ME!\n",
    "input_threat_list_path = (f\"./data/TEAM 3/{round_no}/input_threats.csv\")  # CHANGE ME!\n",
    "# Create new folder for the current round (if not exists)\n",
    "if not os.path.exists(f\"./data/TEAM 3/{round_no}\"):\n",
    "    os.makedirs(f\"./data/TEAM 3/{round_no}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can proceed and run the next cell within the correct path to compute the semantic similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_score, max_score = compute_semantic_similarity_scores(\n",
    "    input_threat_list_path, f\"./data/TEAM 3/{round_no}/semantic_similarity_scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, especially if the number of input threats is large, you can compute the semantic similarity scores by running the `compute_tuple_similarity_scores.py` script in a Terminal:\n",
    "\n",
    "```sh\n",
    "python3 compute_tuple_similarity_scores.py --k <k> --in_path ./data/input_threats.csv [--scores_path ./data/input_threats_semantic_similarity_scores.csv] --out_path ./data/output_similarity_scores.csv\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "- --k <k>: Specifies the tuple cardinality for similarity scoring.\n",
    "- --in_path: Path to the input threats CSV file.\n",
    "- --scores_path: Path to the precomputed semantic similarity scores CSV (only needed if k > 2).\n",
    "- --out_path: Path to the output file for the computed similarity scores.\n",
    "\n",
    "The script will compute the semantic similarity scores and store them into the output path provide (e.g., `./data/TEAM 3/{input_threats_filename}_ss_scores.csv`).\n",
    "\n",
    "Once the script terminates, you can load the semantic similarity scores as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_scores_df = pd.read_csv(f\"./data/TEAM 3/{round_no}/semantic_similarity_scores.csv\")\n",
    "print(f'Total number of input threats submitted for {round_no} round: {len(ss_scores_df.index)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embraceable Candidates Elicitation\n",
    "\n",
    "Please set your desirable semantic similarity score threshold by running the following cell and adjusting the slider according to your target number of desiderable final threats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ss_threshold = widgets.FloatSlider(value=0.5,min=min_score, max=max_score, step=0.01, description='threshold:', readout_format='.2f')\n",
    "ss_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embraceable_candidates = filter_dataframe_by_threshold(ss_scores_df, 'score', ss_threshold.value)\n",
    "print(f'The list of embraceable candidates above the threshold {round(ss_threshold.value, 2)} contains {len(embraceable_candidates)} pairs.')\n",
    "embraceable_candidates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you reached your desiderable amount of embrace candidates, just run the following cell to store the data and proceed to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embraceable_candidates.to_csv(\n",
    "    f\"./data/TEAM 3/{round_no}/embraceable_candidates_{round(ss_threshold.value, 2)}.csv\",\n",
    "    index=True,\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "merge_csv_files(\n",
    "    input_threat_list_path,\n",
    "    f\"./data/TEAM 3/{round_no}/embraceable_candidates_{round(ss_threshold.value, 2)}.csv\",\n",
    "    f\"./data/{round_no}/tmp.csv\",\n",
    ")\n",
    "embraceable_candidates = pd.read_csv(\n",
    "    f\"./data/TEAM 3/{round_no}/embraceable_candidates_{round(ss_threshold.value, 2)}.csv\"\n",
    ")\n",
    "embraceable_candidates.set_index(\"index\", drop=True, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threat Embracing\n",
    "\n",
    "Now you are able to further investigate all the threat pairs with a score equal or greater to such a threshold. To display the table, please run the following cell. Then, you should iterate for each pair candidate and annotate the embracing in an external (Excel-like) sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embraceable_candidates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each row of interesed, specify its index and run the following cell to focus the analysis on such a specific threat pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Please specify the index of the pair you want to embrace.\n",
    "index_to_embrace = 70  # CHANGE ME!\n",
    "embraceable_candidates.iloc[embraceable_candidates.index==index_to_embrace]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you shifted the focus of the analysis on a specific threat pair, run the following cell to obtain automatically identified (if present) synset relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s1 = embraceable_candidates.iloc[embraceable_candidates.index==index_to_embrace]['sentence1'].values[0]\n",
    "s2 = embraceable_candidates.iloc[embraceable_candidates.index==index_to_embrace]['sentence2'].values[0]\n",
    "is_partof, is_typeof = find_synset_relations(s1, s2)\n",
    "print(f'\\nPart of relation(s) found: {is_partof}\\tType of relation(s) found: {is_typeof}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To support the most appropriate choice of wording/level of detail, run the last cell for an overview of the synset relations related to the nouns identified in both threat labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "synset_dict1 = synset_relations(embraceable_candidates.iloc[embraceable_candidates.index==index_to_embrace]['sentence1'].values[0])\n",
    "synset_dict2 = synset_relations(embraceable_candidates.iloc[embraceable_candidates.index==index_to_embrace]['sentence2'].values[0])\n",
    "\n",
    "focus_df = pd.concat([pd.DataFrame.from_dict(synset_dict1['terms']), pd.DataFrame.from_dict(synset_dict2['terms'])])\n",
    "if not focus_df.empty:\n",
    "    focus_df['synonyms'] = focus_df.get('synonyms').str.slice(0,3)\n",
    "    focus_df['hypernyms [L1]'] = focus_df.get('hypernyms [L1]').str.slice(0,1)\n",
    "    focus_df['hyponyms [L1]'] = focus_df.get('hyponyms [L1]').str.slice(0,1)\n",
    "    focus_df['meronyms'] = focus_df.get('meronyms').str.slice(0,1)\n",
    "    focus_df['holonyms'] = focus_df.get('holonyms').str.slice(0,1)\n",
    "    focus_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store Synset Relations Extraction from Embraceable Candidates\n",
    "\n",
    "You can also store the synset relations in batch by running the following cell, should you need them for a future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "# Reference to current sdtout\n",
    "old_stdout = sys.stdout\n",
    "\n",
    "# Create new folder for the current round (if not exists)\n",
    "if not os.path.exists(f'./data/{round_no}/synset_relations'):\n",
    "    os.makedirs(f'./data/{round_no}/synset_relations')\n",
    "\n",
    "for i, pair in embraceable_candidates.iterrows():\n",
    "    path = f'./data/{round_no}/synset_relations/pair_comparison_{i}.txt'\n",
    "    sys.stdout = open(path, 'w')\n",
    "\n",
    "    s1 = embraceable_candidates.iloc[embraceable_candidates.index==i]['sentence1'].values[0]\n",
    "    s2 = embraceable_candidates.iloc[embraceable_candidates.index==i]['sentence2'].values[0]\n",
    "    is_partof, is_typeof = find_synset_relations(s1, s2)\n",
    "    print(f'\\nPart of relation(s) found: {is_partof}\\tType of relation(s) found: {is_typeof}')\n",
    "\n",
    "    sys.stdout = old_stdout\n",
    "\n",
    "    synset_dict1 = synset_relations(embraceable_candidates.iloc[embraceable_candidates.index==i]['sentence1'].values[0])\n",
    "    synset_dict2 = synset_relations(embraceable_candidates.iloc[embraceable_candidates.index==i]['sentence2'].values[0])\n",
    "\n",
    "    focus_df = pd.concat([pd.DataFrame.from_dict(synset_dict1['terms']), pd.DataFrame.from_dict(synset_dict2['terms'])])\n",
    "    if not focus_df.empty:\n",
    "        focus_df['synonyms'] = focus_df.get('synonyms').str.slice(0,3)\n",
    "        focus_df['hypernyms [L1]'] = focus_df.get('hypernyms [L1]').str.slice(0,1)\n",
    "        focus_df['hyponyms [L1]'] = focus_df.get('hyponyms [L1]').str.slice(0,1)\n",
    "        focus_df['meronyms'] = focus_df.get('meronyms').str.slice(0,1)\n",
    "        focus_df['holonyms'] = focus_df.get('holonyms').str.slice(0,1)\n",
    "        focus_df.to_csv(f'./data/{round_no}/synset_relations/single_terms_{i}.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Reset stdout\n",
    "sys.stdout = old_stdout"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
